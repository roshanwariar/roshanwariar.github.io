---
layout: post
title: "Tracing Tokens Through Transformers"
date: 2025-12-16
image: 
---

![Strange Attractor](/assets/images/Poisson_saturne_revisited.jpg)

I have always been interested in physics and AI. That's why the attempt to understand neural networks as dynamical systems has always fascinated me. The first paper I read that really established this connection was Neural Ordinary Differential Equations by Chen, et al. [1]. In this work, they show that residual networks are essentially ordinary differential equations (ODEs) in the limit of infinite depth. A key innovation in these networks were skip connections shown below. 

![Hidden Layer Skip Connection](/assets/images/hidden_layer_skip_connection.png)

If you view each layer as a step forward in time \$t\$, then the hidden layers evolve in the following way:

$$ h_{t+1} = h_{t} + f(h_{t}) $$

The key insight here is that this looks like _Euler's Method_ - a simple numerical way to solve differential equations. In fact, if you allow the number of layers to approach infinity then the hidden layers look like an ODE. 

$$ h(T) = h(0) + \int_{0}^{T} f(h(t), t)\ dt $$

Instead of learning individual transformations from layer to layer, the residual network, through gradient descent training, learns a "velocity field" to efficiently transport its input to an output. Your input can be seen as a particle (vector) moving through this velocity field. <br> 

The extension of this analogy to transformers is that transformers act on multiple tokens (each a particle), and the prompt represents some input distribution over tokens. The mathematics of a transformer is then described best by the mathematics of statistical optimal transport. This field of mathematics is primarily concerned with the least "effort" to convert one probability distribution into another. These distributions exist on an infinite dimensional manifold and distances (defined by geodesics) between distributions are 




[1] Chen, et al. https://arxiv.org/pdf/1806.07366
[2] Rigolette, https://arxiv.org/pdf/2512.01868
