---
layout: post
title: "PISA - Probabilistic Inference-time Scaling Algorithm of LLM Reasoning"
date: 2025-12-15
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together novel reasoning traces that are otherwise not present in the base model's distribution? That is what this year's NeurIPS Best Paper Award runner-up tries to answer. [1]  Simply, their conclusion is _no_. They say, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the "correct" chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they can show for virtually all cases that the base model's accuracy eventually passes the RLVR-trained model's. 



[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
[4]
