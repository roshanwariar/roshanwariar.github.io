---
layout: post
title: "PISA - Probabilistic Inference-time Scaling Algorithm For Extending Language Model Reasoning"
date: 2025-12-8
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together reasoning traces that are otherwise not present in the base model's distribution? That was what one of this year's NeurIPS Best Paper Award runner-up tried to answer [1]. Simply, their conclusion was _no_. They said, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the more rewarding chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they show that base model's accuracy eventually passes the RLVR-trained model's in virtually all cases. Low-temperature sampling of the base model seems to keep chains coherent while preserving diversity such that accuracy becomes quite high at large values of k. 

![RLVR Pass@k](/assets/images/rlvr_results.png)

Now, it's possible this generalization about RLVR is true only when applied to a base model who's single-shot accuracy is not great, or in other words: maybe novel reasoning traces are an emergent property of applying RLVR on a high-performing baseline (large mixture-of-experts). But, this does lend credence to the idea that RLVR _tilts_ the base model's distribution to upweight correct reasoning rather than adding undiscovered traces. RLVR methods like Proximal Policy Optimization (PPO) or the more recent Group Relative Policy Optimization (GRPO) all follow the same optimization problem of tilting the base distribution without straying too far. That objective is formulated with the following equation: 

$$\pi_{\theta}^{*} = \underset{\pi_{\theta}}{\text{argmax}}\ \mathbb{E}_{x\sim D,y \sim \pi_{\theta}(y|x)} \left[ r(x,y) - \beta D_{KL} \big(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)\big) \right]$$

Here, \$\pi_{\theta}^{*}\$ is your final optimized policy or the post-trained language model with parameters \$\theta\$. The right side basically says that you need to find a policy (model) \$\pi_{\theta}\$ that maximizes your reward \$r(x,y)\$ given a response drawn from the model \$y\sim\pi_{\theta}(y\|x)\$ and prompt drawn from a curated dataset \$x\sim D\$. The \$\beta D_{KL}(\pi_{\theta}(y\|x)\parallel\pi_{\text{ref}}(y\|x))\$ term measures the "difference" between the original model distribution \$\pi_{\text{ref}}\$ and the new distribution (policy) through the KL Divergence operation (\$D_{KL}\$) scaled by \$\beta\$. This is subtracted from the reward so to prevent the policy from bifurcating too heavily from the reference base distribution.

When I was doing research during my Master's in probabilistic hardware, I became aware of a lot of powerful sampling algorithms like Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC or particle filtering). If it's true that RLVR is only improving sampling efficiency from the base model's distribution, then I thought it would be interesting to see if these techniques could be applied at test-time to generate reasoning traces similar to what RLVR achieves through post-training. That's exactly what Karan and Du do in their paper "Reasoning with Sampling..." [2].<br>

The algorithm here is pretty straightforward, they generate a block of tokens from a base model (via low-temperature sampling) with size \$B\$ and then choose random points (number of MCMC steps) from the beginning of the solution (right after the prompt) to right before the end of the current block (\$B-1\$). Then they resample from the chosen points and accept the new sequence \$\mathbf{x}'\$ only if its acceptance ratio is less than a randomly drawn number between 0 and 1. The acceptance ratio as defined by the Metropolis-Hasting's algorithm is as follows: 

$$A(\mathbf{x},\mathbf{x}') = \text{min} \bigg[ 1, \frac{\pi(\mathbf{x}')}{\pi(\mathbf{x})} \frac{p_{prop}(\mathbf{x}\|\mathbf{x}')}{p_{prop}(\mathbf{x}'\|\mathbf{x})}\bigg]$$

This is a pretty standard formulation with \$p_{prop}\$ representing the proposal distribution (the low-temperature sampling of the language model). However, I think the real insight is the choice for the target distribution: \$\pi(\mathbf{x})\$ which is proportional to the "sharpened" version of the proposal distribution \$p(\mathbf{x})^\alpha\$ where \$\alpha\ \gt 1$. They call it _power sampling_, and their intuition is that this method upweights tokens who's future tokens will be of higher likelihood. This is laid out more concretely in _Observation 1_ in the paper.<br>

For the first experiment in this blog, I wanted to see if I could replicate the algorithm from this paper but with a larger, newer model and a much harder benchmark. The paper uses Qwen2.5 7B on MATH500. They show that for greedy sampling the model gets 49.8%, 62.8% with low-temperature sampling, 70.6% with power sampling (\$\alpha\$ = 4.0, 10 MCMC steps), and finally 74% for the GRPO-ed version. For pass@2, the base model with power sampling already surpasses the RLVR-tuned model. 

![Pass@k Perf](/assets/images/pass@k_perf.png)

My setup used the Qwen3-14B model with no RL post-training (that I am aware of) on the AIME25 benchmark. The initial experiment involved running the model with 3 configurations: greedy sampling, low-temperature sampling (\$t = 0.25\$) and what I call high-temperature sampling (\$t=0.6\$). This last setting is actually what the Qwen team recommends for this model [3]. The max new tokens was limited to 4096, and everything was run in Google Colab with an Nvidia A100 80GB. Most runs scores are avg@3. Respectively, the scores were 14.45%, 22.22%, and 17.78%. 

![Qwen3 Sampling Performance](/assets/images/qwen3_14b_sampling_aime25.png)

The results suggest that there is a big boost in performance from just going from greedy to low-temperature sampling, and this is substantiated by the results in [2]. Keeping the same temperature, I ran power_sampling_mcmc.py (from the PISA repo on my Github) borrowing some common parameters from the paper. In order to keep the same amount of output tokens (4096), I chose a block size of 256 tokens with 16 blocks. Each block gets 10 MCMC steps and an alpha of 4 to sharpen the base distribution. The below graph shows that this method beats typical temperature sampling by a good amount (27.78%). 

![Qwen3 Power Sampling Performance](/assets/images/qwen3_14b_sampling_aime25_power.png)

Now if you read the code, you might notice that the acceptance ratio is calculated quite differently from the equation above and from the paper. The reason for this because of an interesting simplification that can be done. It starts by recognizing that \$\mathbf{x} = \[\mathbf{x}\_{prefix},\mathbf{x}\_{suffix}]\$  and \$\mathbf{x'} = \[\mathbf{x}\_{prefix},\mathbf{x'}\_{suffix}]\$. Only the suffixes change between the resampled portion and the original answer.  This means that conditional distributions like \$p_{prop}(\mathbf{x}\|\mathbf{x}')\$ are asking: If I have the prefix of the mutated sequence (which is the same for the original sequence), then what is the probability I generate the original sequence? But this ends up just being \$p(\mathbf{x})\$. This reduction then leads to the following new equation for the acceptance which has a nice intuitive explanation: 

$$
\begin{aligned}
A(\mathbf{x}', \mathbf{x}) &= \min \left\lbrace 1, \frac{\pi(\mathbf{x}')}{\pi(\mathbf{x})} \cdot \frac{p_{\text{prop}}(\mathbf{x} \mid \mathbf{x}')}{p_{\text{prop}}(\mathbf{x}' \mid \mathbf{x})} \right\rbrace \\
&= \min \left\lbrace 1, \frac{p_{\text{LM}}(\mathbf{x}')^\alpha}{p_{\text{LM}}(\mathbf{x})^\alpha} \cdot \frac{p_{\text{LM}}(\mathbf{x})}{p_{\text{LM}}(\mathbf{x}')} \right\rbrace \\
&= \min \left\lbrace 1, \left( \frac{p_{\text{LM}}(\mathbf{x}')}{p_{\text{LM}}(\mathbf{x})} \right)^\alpha \cdot \left( \frac{p_{\text{LM}}(\mathbf{x}')}{p_{\text{LM}}(\mathbf{x})} \right)^{-1} \right\rbrace \\
&= \min \left\lbrace 1, \left( \frac{p_{\text{LM}}(\mathbf{x}')}{p_{\text{LM}}(\mathbf{x})} \right)^{\alpha - 1} \right\rbrace
\end{aligned}
$$

Basically, we compared the new suffix to the old one and ask if it's more likely. If it is then you will always accept it. If it's less then depending on the value of alpha there's a chance you may accept it. For example, if the ratio of the logprob sum of the resampled suffix to the original suffix is 0.9 with an alpha of 4 then \$A(\mathbf{x'},\mathbf{x})=0.9^4=0.6561\$ which means there's a 65.61% chance you will take the new suffix.<br>

The big drawback of power sampling with MCMC is the runtime, and this was demonstrated in both the paper and my own experiments. While RL priotizes single-shot accuracy and post-training can be expensive, it is a one-time cost to get the model to reason better. From then on, inference speed is just as fast the original model (unless you need to generate signifcantly more answer tokens to achieve correctness). Power sampling needs to be run at inference time, and it's autoregressive nature makes it incredibly slow (almost an hour per question). Theoretically, its expected to generate \$M * T^2 * \frac{1}{4B}\$ tokens where T is the token length of the base model, M is the number of MCMC steps, and B is the block size. The answer size is maxed out at 4096, so in the worst case, our model generates something like 16 million tokens before its done answering. In practice, generated tokens (answer + resampling) were usually 9-10X of the base model's answer length. <br> 

To get around something like this, I thought about population-based sampling methods like Sequential Monte Carlo. If we could run this over multiple chains that explore different token sequences in order to guide the model to spend compute on more promising chains in parallel then maybe you could get the intelligence boost of power sampling without spending 25-30 hours to complete AIME25 benchmark. The faster runtime might also allow me to scale this method further. Given, the pass@k performance of base models shown above, there's good reason to believe that probabilistic multi-chain exploration yields better intelligence/FLOP than power sampling with MCMC which focuses on sharpening a single chain. <br> 

Using Sequential Monte Carlo as an probabilistic inference-time scaling algorithm is not new. The paper Rollout Roulette by Puri, et al. [4] looks at using lanaguage models as a proposal mechanism and a math process reward model to grade parallel chains-of-thought traces with scores which are normalized and used as weights for SMC. They show that this method achieves better intelligence for the same number of rollouts compared to best-of-N, weighted BoN, and diverse verifier tree search. Personally, the idea of using a PRM isn't very elegant and while the paper shows that using a simple math PRM for SMC can extend to other reasoning domains, I wanted to see how far we could push the performance of the base model without a critic model or external tools.<br> 

![Beam Search](/assets/images/beam_search.png) 

SMC or particle filtering is actually quite similar to beam search with the difference being that the weights of each CoT form a distribution over which you sample to duplicate chains (particles). This is essentially stochastic beam search. In [4], they let the model run a "step" which could be any amount of tokens and then use a separate model to grade the quality of that step. This does not really work if you don't have a PRM. Instead, my idea was to have the model generate 256 token blocks (for each chain) similar to power sampling MCMC and then use the sum of the logprobs as the weight value. I initially experimented with average logprob, but that seemed to upweight generic fluff in the answers probably because math jargon is often lower likelihood in the vocab for next token prediction. <br> 

Another issue I had with my initial setup was the temperature of 0.25. It lead a vast majority of the different particles to be the exact same sequence (and I was unable to get scores better than the 27.78% achieved by power sampling with MCMC). So I chose a temperature of 0.6, so that chains would be more diverse. In order to make sure that only the best chains were propagated I borrowed an idea from power sampling. When softmaxing the scores to define a distribution over the chains, I used an alpha value to sharpen the distribution such that chains with slightly better logprob sums get weighted even higher when it comes time to sample. The other parameters were a block size of 256 tokens, 64 particles, and an alpha of 4. Note: I totally understand the limitations with using something like logprob sum of sequence as a heursitic for scoring since models can be confidently incorrect, but again I just wanted to see how far I could push the model with no external validation signals. <br> 

The experiment was significantly faster than power sampling with autoregressive MCMC (1.5 vs 24-26 hrs for entire AIME25). You can run the experiment using the power_sampling_smc.py file in the PISA repo. It was also the first experiment that allowed the model to suppass the 30% barrier by getting 33.33%. Additionally, when I pushed it to 128 particles in parallel, runtime barely increased, but the model achieved an accuracy of 35.56% (avg@3). I think it's pretty astounding that we have been able to increase the base model's reasoning capabilties by simply using a combination of scoring heuristics and probabilistic algorithms. No verifier and no curated dataset!

![Final Sampling Results](/assets/images/sampling_performance_final.png)

This kind of performance, specifically for a model like Qwen3-14B to get nearly 36% on AIME25 is even more impressive when put much larger, capable models into perspective. One graph that caught my eye when reading the Qwen3 Technical Report [5] was the one below. It shows that a large mixture-of-experts model like Qwen3-235B-A22B with RL post-training achieves around 42% on AIME25 when given a thinking budget of 4k tokens (what we capped all of our experiments at). 

![](/assets/images/thinking_qwen3_tech_report.png)

I think methods like this extend the possiblities of bringing reasoning models to mobile devices or edge sensors. A laptop with 32GB could fit an FP8 version of Qwen3-14B easily, and these inference-time scaling methods show that you can get within striking distance of the performance of much larger reasoning models for which you may need multiple expensive GPUs to run. 

In summary, we showed that power sampling with autoregressive MCMC still generates better reasoning capabilites (than low-temperature sampling) for larger, newer models on hard benchmarks (less than 50% accuracy for the base models). We adapted power sampling concepts to a kind of stochastic beam search algorithm that allowed us to extend the performance of the base model close to that of RL post-trained models that have several architectural advantages (parameter count, mixture-of-experts). Thanks for reading!
 
[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Qwen3-14B Model Info. https://huggingface.co/Qwen/Qwen3-14B<br>
[4] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
[5] Qwen3 Technical Report. https://arxiv.org/pdf/2505.09388
