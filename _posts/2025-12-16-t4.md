---
layout: post
title: "Tracing Tokens Through Transformers"
date: 2025-12-16
image: 
---

![Strange Attractor](/assets/images/Poisson_saturne_revisited.jpg)

I have always been interested in physics and AI. That's why the attempt to understand neural networks as dynamical systems has always fascinated me. The first paper I read that really established this connection was Neural Ordinary Differential Equations by Chen, et al. [1]. In this work, they show that residual networks (ResNets) are essentially ordinary differential equations (ODEs) in the limit of infinite depth. A key innovation in these networks were skip connections shown below. 

![Hidden Layer Skip Connection](/assets/images/hidden_layer_skip_connection.png)

If you view each layer as a step forward in time \$t\$, then the hidden layers evolve in the following way:

$$ h_{t+1} = h_{t} + f(h_{t}) $$

The key insight here is that this looks like _Euler's Method_ - a simple numerical way to solve differential equations. In fact, if you allow the number of layers to approach infinity then the hidden layers look like an ODE. 

$$ h(T) = h(0) + \int_{0}^{T} f(h(t), t)\ dt $$

Instead of finding individual transformations from layer to layer, the ResNet, through gradient descent training, learns a "velocity field" to efficiently transport its input to an output. Your input can be seen as a particle (vector) moving through this velocity field. <br> 

The extension of this analogy to transformers is that transformers act on multiple tokens (each a particle), and the prompt represents some input distribution over tokens. The dynamics of a transformer is then described best by the mathematics of statistical optimal transport. This field is primarily concerned with finding the least "effort" to convert one probability distribution into another. These distributions exist on an infinite dimensional manifold and distances between distributions (defined by geodesics in Wasserstein space) encode the amount of work to change distribution A to B. <br>

Transformers have two key computational components, self-attention and MLP (ResNet) layers. We have already made the connection to show how ResNet layers act like discrete time steps in the prescence of a velocity field. As shown by many, including Rigollet [2], self-attention acts like a the mean field dynamics of a spin-glass or Ising system from statistical mechanics. This model was used to describe the phase transitions of ferromagnetic materials. Without an external field, the energy (Hamiltonian) of an Ising system is described as: 

$$ H = \sum_{i,j} J_{ij}\sigma_{i}\sigma_{j} $$

Where $$\sigma_{i}$$ is the $$i$$-th spin and $$J_{ij}$$ represents the interaction or coupling strength between two neighboring spins. The system naturally sets spins to minimize the energy of the magnetic fields of the whole system. For tokens, the query of the $$i$$-th token has its similarity to the key of the $$j$$-th token calculated via the dot product: 

$$ {\Large <}Qx_{i}, Kx_{j}{\Large >} $$

A high value for the dot product means a high similarity and a low energy. This corresponds with a high attention weight. In other words, tokens "want" to align with other tokens just like neighboring spins. <br> 

So this takes us to the experiment for this post. Simply put, I wanted to trace the predicted next-word token through the transformer layers given a specific prompt and see what it teaches us about its dynamics. The next step was to pass in multiple different prompts, trace the trajectories of the tokens, project those trajectories down into a 2D plot using PCA, and see if any bifuractions occur that tell us about the model accessing semantically meaningful parts of its knowledge manifold based on the input prompt. <br> 

I asked Gemini to generate a few prompts per category and then extracted the final tokens' activations from each layer for each prompt, squeezed them into a single tensor, then did a PCA projection to two dimensions. The model was Qwen3-4B-Instruct-2507 and it has 36 layers. An issue I encountered was that the last layer uses a RMS layer normalization that makes the PCA plot more confusing to follow. For that reason, I excluded the last layer's activations from the PCA. All experiments are run on Google Colab with an Nvidia A100 80GB GPU. 

The following categories and prompts were used. <br> 

_Math_:<br> 
        "Solve: 15 * 12 + 4 =",<br>
        "Calculate the area of a circle with radius 5:",<br>
        "If 3x + 5 = 20, then x is",<br>
        "The square root of 144 is",<br>
        "Integrate 2x dx from 0 to 5"<br>
        
_Code_:<br> 
        "def fibonacci(n):",<br>
        "print('Hello World')",<br>
        "import numpy as np",<br>
        "for i in range(10):",<br>
        "class Transformer(nn.Module):"<br>
        
_History/Facts_:<br>
        "The capital of France is",<br>
        "The first US president was",<br>
        "World War II ended in",<br>
        "Water boils at 100 degrees",<br>
        "The largest planet is"<br>

_Creative/Poetry_:<br>
        "The moon whispered to the",<br>
        "Once upon a time in a land of",<br>
        "Write a haiku about leaves:",<br>
        "Her eyes were like deep",<br>
        "In the silence of the night,"<br>
        
_Logic/Syllogism_:<br>
        "All men are mortal. Socrates is a man. Therefore,",<br>
        "If it rains, the grass gets wet. It rained. Thus,",<br>
        "A > B and B > C, so A is",<br>
        "Either Red or Blue. Not Red. Therefore,",<br>
        "All birds have wings. A penguin is a bird. So,"<br>

And, the result was the following graph: 

![Initial Test](/assets/images/initial_prompt_test_pca.png)

I think this is a pretty incredible result for a couple of reasons. The first reason is that the two PCA dimensions clearly encode characteristics of the tokens dynamics with principal component 1 (PC1) describing the "time" dimension - ie tokens flowing through the layers of the transformer, and PC2 describing the language space. As I will do later on, you can even get a sense of the "velocity" of the tokens moving through the language space by taking the derivative of PC2 (position) with respect to PC1 (time). This graph actually looks quite similar to one of the images from the Neural ODE paper [1] that I think affirms the analogy. <br> 

![Neural ODE flipped](/assets/images/residual_network_visual.png)

The second reason I think this is a cool graph, is because we clearly see groupings of prompts together based on some similarity prompts have with other prompts in their own category as well as other categories. My sense is that we can explain the groupings based on the hypothesis that the model learns to differentiate between natural and formal language. Specifically, prompts that exist in the positive region of PC2 are understood by the model to be natural language and negative PC2 areas are what the model thinks is formal language (programming).<br>

This is first seen with the code prompts (black) which shoot into the strongly negative region of PC2. The red lines corresponding to the math prompts sit around a PC2 value of 0 which might indicate the model thinks that the prompt is neither fully natural language nor formal, but rather something that combines both. This makes sense since the math prompts I used were word problems combining natural language words with mathematical symbolic notation. <br> 

History facts and creative poetry both appear as the most positive PC2 values which makes sense since they are only natural language, but what I think is interesting is the logic prompts. For the most part, these are purely natural language (except for the one prompt that uses greater than/less than symbols), but the model places them in its own distinct region away from the creative writing. I believe this could also be due to the use of "programming-like" key words and structures like IF, THEN, OR, AND etc which allows for the model to realize its looking at text more structured than natural language, but maybe not so much as math word problems. <br> 

To take this idea further, I thought we could try and trick the model by giving it formal language, math problems, and creative writing prompts as well as weird inbetween prompts to see how it thought about each and if we could see more of a spectrum of thought from the model. Here were the new set of prompts I got with the help of Gemini.<br> 

_Pure Code_:<br>
1:        "def quicksort(arr):",<br>
2:        "import torch.nn as nn",<br>
3:        "while True: print('loop')",<br>
        
_Pure Math_:<br>
4:        "15 * 44 + 12 =",<br>
5:        "Calculate the integral of 2x:",<br>
6:        "Solve for x: 3x - 5 = 10",<br>
        
_Pure Story_:<br>
7:        "Once upon a time in a castle",<br>
8:        "The sun set over the horizon",<br>
9:        "She whispered into the wind",<br>
        
__Now, the weird inbetween stuff.__<br>

_Word Problems_:<br>
10:        "If John has 50 apples and gives 12 to Mary, he has",<br>
11:        "A train leaves Chicago at 60mph. Two hours later,",<br>
12:        "Mary is twice as old as John. If John is 10, Mary is"<br>
        
_Docstrings_:<br>
13:        "This function calculates the fibonacci sequence by", --> English-heavy<br>
14:        "# The following code connects to the database and",  --> Comment style<br>
15:        "''' This class handles user authentication '''"      --> Code-syntax style<br>

_Pseudocode_: <br>
16:        "IF user is valid THEN grant access ELSE",<br>
17:        "REPEAT until x is greater than 10:",<br>
18:        "SET counter to zero and INCREMENT by one"<br>

_Algorithmic Poetry_:<br>
19:        "class Love(Emotion): def \_\_init\_\_(self):",<br>
20:        "while heart.is_beating(): print('I miss you')",<br>
21:        "if (tears > 0): return sadness else: return hope"<br>

The resulting graph with numbers corresponding to the prompts is shown below. 

![Spectrum of Thought](/assets/images/spectrum_of_thought.png)

Here, the PC2 values have been reversed in the sense that negative values now correspond with more natural language and positive values are formal language. The graph shows that the experiment worked by creating a clear cluster for the pure story prompts and pure code prompts, but now with a spectrum of inbetween prompts that highlights how the model undestands different formatting. Let's break these down by category.<br> 

We can see the pure story prompts grouped together at the bottom (blue 7,8,9). The next grouping is the word problems just above (10,11,12). These are mostly natural language, but they do contain math symbols namely numbers. The model already understands that the language here is more structured, and therefore it's not near the natural language prompts like pure story. However, when comparing the distance from pure story to word problems and word problems to pure math (red prompts 4,5,6), the word problems are closer to natural language even though the problem itself requires a set of tools and knowledge much closer to the pure math prompts.<br> 

And I wondered, but didn't get a chance to fully explore, if this is the reason base LLMs seem to do worse at mathematical tasks encoded in natural language vs. pure math benchmarks. There has been a debate around whether LLMs actually do mathemtical reasoning especially with Apple's GSM-Symbolic paper from last year [3]. They argued that maybe LLMs do not reason beyond memorization by showing that if you choose random parameters for mathematical word problems from the GSM8K benchmark, it's accuracy falls.<br> 

This contradicts what has felt like robust reasoning skills across a number of other benchmarks like MATH and AIME for the same models that the paper by Mizradeh, et al tested. This is pure speculation, but if the model access different parts of its training data or knowledge manifold to solve a particular problem, then _how_ the model sees a type of question would drastically change its performance. And here, it seems like instead of thinking of word problems similarly to math problems, thus accessing its knowledge about math, it sees the prompt closer to creative writing and probably relies on memorization to answer the question explaining the GSM-Symbolic paper's results.<br>

An interesting follow-up experiment would be to see if it's possible to increase the model's accuracy at math word problems by dynamically steering the prompt in the layers to the pure math region. That may also destroy the internal model's understanding of the prompt, so it's something I need to look into more. Instead you can get an intuitive feel of the dynamics from this gif :)<br>

![Thought Evolution](/assets/images/thought_evolution.gif)

The next group of categories that is interesting to point out is the docstrings and the pseudocode. Prompts 13,14 from the docstrings and 16,18 from psuedocode are clumped together between math word problems and pure math problems. I think it's useful to try and explain why 15 and 17 are in totally different regions, so let's start with 15.<br>

Unlike 13 and 14, prompt 15 sits right up with the pure code examples. Not only that, but in the earlier layers, prompt 15 undergoes substantial change (large velocity) in just one or two layers that causes it to significantly bifurcate from the rest of the docstring prompts. This is peculiar because prompt 15: ''' This class handles user authentication ''' is mostly natural language except for the ''' delinating a class definition in Python. Prompt 13: "This function calculates the fibonacci sequence by" could also be a class docstring but without the ''' it is seen as moreso natural language.<br> 

Prompt 17 being far more pure math-like compared to the other pseudocode examples is most likely because instead of spelling out the word "ten" like Prompt 18 does with "one" and "zero", it uses "10". This pushes the prompt more into the pure math region than the other psuedocode examples in the same way that word problems are pushed away from creative writing more to pure math. <br>  

Anyways, that's probably the end of this blogpost. Visualizing the internal dynamics of LLMs was cool and looking at the differences between prompts and how different layers separates those differences also is illuminating. I think this could lead to some interesting internal LLM steering projects in the future. Cheers. 


[1] Chen, et al. https://arxiv.org/pdf/1806.07366<br>
[2] Rigollet, https://arxiv.org/pdf/2512.01868<br>
[3] Mizradeh, et al. https://arxiv.org/pdf/2410.05229v1
