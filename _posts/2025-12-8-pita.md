---
layout: post
title: "PISA - Probabilistic Inference-time Scaling Algorithm For Extending Language Model Reasoning"
date: 2025-12-8
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together reasoning traces that are otherwise not present in the base model's distribution? That was what this year's NeurIPS Best Paper Award runner-up tried to answer. [1]  Simply, their conclusion was _no_. They said, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the "correct" chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they show that base model's accuracy eventually passes the RLVR-trained model's in virtually all cases. 

![RLVR Pass@k](/assets/images/rlvr_results.png)

Now, it's possible this generalization about RLVR is true only when applied to a base model who's single-shot accuracy is not great, or in other words: maybe novel reasoning traces are an emergent property of applying RLVR on a high-performing baseline (large mixture-of-experts). But, this does lend credence to the idea that RLVR _tilts_ the base model's distribution to upweight correct reasoning rather than adding undiscovered traces. RLVR methods like Proximal Policy Optimization (PPO) or the more recent Group Relative Policy Optimization (GRPO) all follow the same optimization problem of tilting the base distribution without straying too far. That objective is formulated with the following equation: 

$$\pi_{\theta}^{*} = \underset{\pi_{\theta}}{\text{argmax}}\ \mathbb{E}_{x\sim D,y \sim \pi_{\theta}(y|x)} \left[ r(x,y) - \beta D_{KL} \big(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)\big) \right] \text{(1)}$$

Here, \$\pi_{\theta}^{*}\$ is your final optimized policy which is your fine-tuned language model with parameters \$\theta\$. The right side basically says that you need to find a policy (model) \$\pi_{\theta}\$ that maximizes your reward \$r(x,y)\$ given a response drawn from the model \$y\sim\pi_{\theta}(y\|x)\$ and prompt drawn from a curated dataset \$x\sim D\$. The \$\beta D_{KL}(\pi_{\theta}(y\|x)\parallel\pi_{\text{ref}}(y\|x))\$ term measures the "difference" between the original model distribution \$\pi_{\text{ref}}\$ and the new distribution (policy) through the KL Divergence operation (\$D_{KL}\$) scaled by \$\beta\$. This is subtracted from the reward so to prevent the policy from bifurcating too heavily from the reference base distribution.

When I was doing research during my Master's in probabilistic hardware, I became aware of a lot of powerful sampling algorithms like Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC or particle filtering). If it's true that RLVR is only improving sampling efficiency from the base model's distribution, then I thought it would be interesting to see if these techniques could be applied to improving 

[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
