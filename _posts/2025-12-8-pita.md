---
layout: post
title: "PISA - _P_robabilistic _I_nference-time _S_caling _A_lgorithm For LLM Reasoning"
date: 2025-12-8
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together novel reasoning traces that are otherwise not present in the base model's distribution? That is what this year's NeurIPS Best Paper Award runner-up tries to answer. [1]  Simply, their conclusion is _no_. They say, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the "correct" chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they can showthat for virtually all cases the base model's accuracy eventually passes the RLVR-trained model's. 

![RLVR Pass@k](/assets/images/rlvr_results.png)

Now it is possible these results are true for a model who's single-shot baseline accuracy is not great, or in other words, maybe novel reasoning traces are an emergent property of applying RLVR on a high-performing reasoning baseline (large mixture-of-experts). But, I think this does lend credence to the idea that RLVR _tilts_ the base model's distribution to upweight correct reasoning rather than adding new traces. RLVR methods like Proximal Policy Optimization (PPO) or the more recent Group Relative Policy Optimization (GRPO) all follow the same optimization problem of shifting the model distribution to reward weighted traces while staying as close as possible to the original model's distribution. That equation is: 

$$\pi_{\theta}^{*} = \underset{\pi_{\theta}}{\text{argmax}}\ \mathbb{E}_{x\sim D,y \sim \pi\_{\theta}(y|x)} [ r(x,y) - \beta \dot D\_{KL} (\pi\_{theta}(y|x) || \pi\_{ref}(y|x))\] \ \text{(1)}$$ 





[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
[4]
