---
layout: post
title: "Tracing Tokens Through Transformers"
date: 2025-12-16
image: 
---

![Strange Attractor](/assets/images/Poisson_saturne_revisited.jpg)

I have always been interested in physics and AI. That's why the attempt to understand neural networks as dynamical systems has always fascinated me. The first paper I read that really established this connection was Neural Ordinary Differential Equations by Chen, et al. [1]. In this work, they show that residual networks (ResNets) are essentially ordinary differential equations (ODEs) in the limit of infinite depth. A key innovation in these networks were skip connections shown below. 

![Hidden Layer Skip Connection](/assets/images/hidden_layer_skip_connection.png)

If you view each layer as a step forward in time \$t\$, then the hidden layers evolve in the following way:

$$ h_{t+1} = h_{t} + f(h_{t}) $$

The key insight here is that this looks like _Euler's Method_ - a simple numerical way to solve differential equations. In fact, if you allow the number of layers to approach infinity then the hidden layers look like an ODE. 

$$ h(T) = h(0) + \int_{0}^{T} f(h(t), t)\ dt $$

Instead of finding individual transformations from layer to layer, the ResNet, through gradient descent training, learns a "velocity field" to efficiently transport its input to an output. Your input can be seen as a particle (vector) moving through this velocity field. <br> 

The extension of this analogy to transformers is that transformers act on multiple tokens (each a particle), and the prompt represents some input distribution over tokens. The dynamics of a transformer is then described best by the mathematics of statistical optimal transport. This field is primarily concerned with finding the least "effort" to convert one probability distribution into another. These distributions exist on an infinite dimensional manifold and distances between distributions (defined by geodesics in Wasserstein space) encode the amount of work to change distribution A to B. <br>

Transformers have two key computational components, self-attention and MLP (ResNet) layers. We have already made the connection to show how ResNet layers act like discrete time steps in the prescence of a velocity field. As shown by many, including Rigollet [2], self-attention acts like a the mean field dynamics of a spin-glass or Ising system from statistical mechanics. This model was used to describe the phase transitions of ferromagnetic materials. Without an external field, the energy (Hamiltonian) of an Ising system is described as: 

$$ H = \sum_{i,j} J_{ij}\sigma_{i}\sigma_{j} $$

Where $$\sigma_{i}$$ is the $$i$$-th spin and $$J_{ij}$$ represents the interaction or coupling strength between two neighboring spins. The system naturally sets spins to minimize the energy of the magnetic fields of the whole system. For tokens, the query of the $$i$$-th token has its similarity to the key of the $$j$$-th token via the dot product: 

$$ {\Large <}Qx_{i}, Kx_{j}{\Large >} $$

A high value for the dot product means a high similarity and a low energy. This corresponds with a high attention weight. In other words, tokens "want" to align with other tokens just like neighboring spins. <br> 

So takes us to the experiment for this post. Simply put, I wanted to trace the predicted token through the transformer layers given a specific prompt and see what it teaches us about its dynamics. The next step was to pass in multiple varying prompts, trace the trajectories of the tokens, project those trajectories down into a 2D plot using PCA and see if any bifuractions occur. 



[1] Chen, et al. https://arxiv.org/pdf/1806.07366
[2] Rigollet, https://arxiv.org/pdf/2512.01868
