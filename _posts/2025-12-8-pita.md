---
layout: post
title: "PISA - Probabilistic Inference-time Scaling Algorithm For LLM Reasoning"
date: 2025-12-8
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together reasoning traces that are otherwise not present in the base model's distribution? That was what this year's NeurIPS Best Paper Award runner-up tried to answer. [1]  Simply, their conclusion was _no_. They said, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the "correct" chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they show that base model's accuracy eventually passes the RLVR-trained model's for virtually all cases. 

![RLVR Pass@k](/assets/images/rlvr_results.png)

Now it's possible this generalization about RLVR is true when applied to a model who's single-shot baseline accuracy is not great, or in other words: maybe novel reasoning traces are an emergent property of applying RLVR on a high-performing baseline (large mixture-of-experts). But, this does lend credence to the idea that RLVR _tilts_ the base model's distribution to upweight correct reasoning rather than adding new traces. RLVR methods like Proximal Policy Optimization (PPO) or the more recent Group Relative Policy Optimization (GRPO) all follow the same optimization problem of tilting the base distribution without straying too far. That objective is formulated with the following equation: 

$$\pi_{\theta}^{*} = \underset{\pi_{\theta}}{\text{argmax}}\ \mathbb{E}_{x\sim D,y \sim \pi_{\theta}(y|x)} \left[ r(x,y) - \beta D_{KL} \big(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)\big) \right] \text{(1)}$$

[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
[4]
