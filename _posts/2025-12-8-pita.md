---
layout: post
title: "PISA - Probabilistic Inference-time Scaling Algorithm For Extending Language Model Reasoning"
date: 2025-12-8
image: 
---

![PISA Pic](/assets/images/500px-Leaning_Tower_of_Pisa_in_the_1890s.jpg)

Recently, I read a couple of papers that have touched on a similar topic I've been interested in. Does the use of reinforcement learning with verifable rewards (RLVR) after pre-training and instruction fine-tuning really help the model string together reasoning traces that are otherwise not present in the base model's distribution? That was what this year's NeurIPS Best Paper Award runner-up tried to answer [1]. Simply, their conclusion was _no_. They said, "our findings reveal that current RLVR does not elicit fundamentally new reasoning patterns; instead, the reasoning capabilities of RLVR-trained models remain bounded by those of their base models." The hypothesis is that RLVR simply trades off diversity for single-shot accuracy (ie exploration vs. exploitation - or more efficient sampling of the "correct" chain-of-thought).<br>

![Explore vs Exploit](/assets/images/explore_v_exploit.png)

By looking at pass@k metrics for high k values across a variety of smaller lanaguage models (up to 32B dense) on popular reasoning benchmarks, they show that base model's accuracy eventually passes the RLVR-trained model's in virtually all cases. Low-temperature sampling of the base model seems to keep chains coherent while preserving diversity such that accuracy becomes quite high at large values of k. 

![RLVR Pass@k](/assets/images/rlvr_results.png)

Now, it's possible this generalization about RLVR is true only when applied to a base model who's single-shot accuracy is not great, or in other words: maybe novel reasoning traces are an emergent property of applying RLVR on a high-performing baseline (large mixture-of-experts). But, this does lend credence to the idea that RLVR _tilts_ the base model's distribution to upweight correct reasoning rather than adding undiscovered traces. RLVR methods like Proximal Policy Optimization (PPO) or the more recent Group Relative Policy Optimization (GRPO) all follow the same optimization problem of tilting the base distribution without straying too far. That objective is formulated with the following equation: 

$$\pi_{\theta}^{*} = \underset{\pi_{\theta}}{\text{argmax}}\ \mathbb{E}_{x\sim D,y \sim \pi_{\theta}(y|x)} \left[ r(x,y) - \beta D_{KL} \big(\pi_{\theta}(y|x) || \pi_{\text{ref}}(y|x)\big) \right]$$

Here, \$\pi_{\theta}^{*}\$ is your final optimized policy or the post-trained language model with parameters \$\theta\$. The right side basically says that you need to find a policy (model) \$\pi_{\theta}\$ that maximizes your reward \$r(x,y)\$ given a response drawn from the model \$y\sim\pi_{\theta}(y\|x)\$ and prompt drawn from a curated dataset \$x\sim D\$. The \$\beta D_{KL}(\pi_{\theta}(y\|x)\parallel\pi_{\text{ref}}(y\|x))\$ term measures the "difference" between the original model distribution \$\pi_{\text{ref}}\$ and the new distribution (policy) through the KL Divergence operation (\$D_{KL}\$) scaled by \$\beta\$. This is subtracted from the reward so to prevent the policy from bifurcating too heavily from the reference base distribution.

When I was doing research during my Master's in probabilistic hardware, I became aware of a lot of powerful sampling algorithms like Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC or particle filtering). If it's true that RLVR is only improving sampling efficiency from the base model's distribution, then I thought it would be interesting to see if these techniques could be applied at test-time to generate reasoning traces similar to what RLVR achieves through post-training. That's exactly what Karan and Du do in their paper "Reasoning with Sampling..." [2].<br>

The algorithm here is pretty straightforward, they generate a block of tokens from a base model (via low-temperature sampling) with size \$B\$ and then choose random points (number of MCMC steps) from the beginning of the solution (right after the prompt) to right before the end of the current block (\$B-1\$). Then they resample from the chosen points and accept the new sequence \$\mathbf{x}^{*}\$ only if its acceptance ratio is less than a randomly drawn number between 0 and 1. The acceptance ratio as defined by the Metropolis-Hasting's algorithm is as follows: 

$$A(\mathbf{x},\mathbf{x}^{*}) = \text{min} \bigg[ 1, \frac{\pi(\mathbf{x}^{\*})}{\pi(\mathbf{x})} \frac{p_{prop}(\mathbf{x}\|\mathbf{x}^{\*})}{p_{prop}(\mathbf{x}^{\*}\|\mathbf{x})}\bigg]$$

This is a pretty standard formulation with \$p_{prop}\$ representing the proposal distribution (the low-temperature sampling of the language model). However, I think the real insight is the choice for the target distribution: \$\pi(\mathbf{x})\$ which is proportional to the "sharpened" version of the proposal distribution \$p(\mathbf{x})^\alpha\$ where \$\alpha\ \gt 1$. They call it _power sampling_, and their intuition is that this method upweights tokens who's future tokens will be of higher likelihood. 
 
[1] Yue, et al. https://openreview.net/pdf?id=4OsgYD7em5<br>
[2] Karan, Du. https://arxiv.org/pdf/2510.14901<br>
[3] Puri, et al. https://arxiv.org/pdf/2502.01618<br>
